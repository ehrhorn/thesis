%!TEX root = ../main.tex
\documentclass[../main.tex]{subfiles}

\begin{document}

Contrary to public expectation these days, machine learning (ML) is neither sexy nor particularly exciting.
The misnomer \enquote{AI} is often applied to ML, evoking expectations of some general computer intelligence that might rise up and replace humans as the Earth's (and soon, the galaxy!) dominant species.

ML is a varied field, with many different subgenres, of which Deep Learning (DL) is the one that this thesis is concerned with.

However, it might be beneficial to first describe what ML as a field is, before delving into the specifics of DL.\@

% All ML is, is good, old, boring, dependable statistics, calculus and linear algebra.
% You need calculus to figure out which way weights must be adjusted, and you need linear algebra to apply computations quickly.

\section{ML basics}\label{sec:ml_basics}

The fundamental schism between traditional programming and ML is this: programs apply logic to inputs, and produce outputs, while ML uses outputs on inputs to produce logic.

Machine learning generally comes in three flavors: supervised learning, unsupervised learning and reinforcement learning.
As supervised learning is the tool used in this thesis, it is the only flavor that we shall discuss.

Generally, supervised machine learning entails having a way to map inputs to outputs (e.g.\ decision trees in tree based ML, artificial neurons in deep learning), access to some labelled dataset and for each data point the ability to calculate an error.
This error is encapsulated in a loss functions, which takes as input the model's guess for the data point value and the true value (the \enquote{label}) and returns a real number, the \enquote{loss}.
The name of the game is then the minimization of this loss, as an error of zero would mean a perfect estimation of the label value.
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{./images/machine_learning/Ea9GZUbXsAEORIb.jpeg}
    \caption{The \enquote{Face-Depixelizer}~\cite{Menon2020} takes a blurred image of a person, and generates an image from it. However, the model was trained on primarily white people, and so reconstructs an obviously blurred image of Barack Obama as a white male. Image from \url{https://twitter.com/Chicken3gg/status/1274314622447820801}}\label{fig:algorithmic_bias}
\end{figure}
The input/output data is referred to as \enquote{training data}, and it is in essence what decides the logic of the model; thus care must be taken with regards to it, as inherent biases in the training data will propagate to the decisions the model makes.
This is a continuation of the old programmer's adage, \enquote{garbage in, garbage out}, because an ML algorithm will only give back what is fed to it; it is not sentient.
This is a very active area of discussion at the time of writing, and the summer of 2020 gave a very good example of the issue; see~\vref{fig:algorithmic_bias}.

Training data is not necessarily easy to come by---and may, for some companies, present an impenetrable moat, if the competition does not have access to the same dataset---but particle physics is special in this regard as Monte Carlo data (\vref{sec:monte_carlo_simulation}) is typically plentiful; however, it should be noted that Monte Carlo data itself is simulated, based on assumptions the laws of physics, and thus represents an inherently biased version of how the universe works.

\todo[inline]{More about ML basics...}

\section{Artificial neural networks}\label{sec:artificial_neural_networks}

Artificial neural networks (ANN) have a much longer story than would probably be thought, seeing how they currently stand as pillars of futurism.
ANNs can be said to draw inspiration from nature as the simplest version, a feedforward neural network, uses artificial neurons, modeled on the neurons of a brain, first proposed in the forties.

The artificial neuron takes \( m + 1 \) inputs, consisting of values \( x_{j} \) weighted by \( w_{k m} \) and applies some \enquote{activation function} \( \phi \) to their sum, such that
\begin{equation}
	y_{k} = \phi \left( \sum_{j = 0}^{m} w_{k j} x_{j} \right).
\end{equation}
The activation function is typically non-linear, e.g.\ a sigmoid
\begin{equation}
	S(x) = \frac{e^{x}}{e^{x} + 1},
\end{equation}
or, the popular choice these days, a rectifier
\begin{equation}
	f(x) = x^{+} = \max{\left( 0, x \right)}.
\end{equation}
\begin{figure}[ht]
    \centering
    \input{./images/machine_learning/activation_functions.pgf}
    \caption{Two activation functions, a sigmoid and a rectifier. Both \enquote{activate} an input, rather like a Heaviside step function.}\label{fig:activation_functions}
\end{figure}
To explain how ANNs work, a simple example using simple linear regression is illuminating.

In linear regression, a 200 year old technique, a linear relationship between \( n \) dependent variables, \( y_{k} \), and input variables \( \bm{x}_{k} \), is assumed, such that the output can be approximated as
\begin{equation}
    y_{k} \approx w_{0} x_{k 0} + w_{1} x_{k 1} + \cdots + w_{m} x_{k m} = \sum_{i = 0}^{m} x_{k i} = \bm{x}_{k}^{\intercal} \bm{w},
\end{equation}
where \( x_{k 0} = 1 \) such that \( w_{0} \) is the bias.
\begin{figure}[ht]
    \centering
    \includegraphics{./images/machine_learning/perceptron.pdf}
    \caption{The linear perceptron.}\label{fig:perceptron}
\end{figure}
This is a linear perceptron, an artificial neuron with a linear activation function; see~\vref{fig:perceptron}, modeled as a directed acyclic graph.
Regression then proceeds by minimizing the error, e.g.\ the sum of squared residuals
\begin{equation}
    \mathcal{L}(\bm{w}) = \sum_{k = 1}^{n} {\left( y_{k} - \bm{x}_{k}^{\intercal} \bm{w} \right)}^2,
\end{equation}
a function of the weights.
We are then able to ascertain, by using the derivative of the loss function with respect to the weights, 
\begin{equation}
    \diffp{\mathcal{L}}{w_{i}} = - 2 \sum_{k = 1}^{n} {\left( y_{k} - \bm{x}_{k}^{\intercal} \bm{w} \right)} x_{k i},
\end{equation}
the relationship between the minimum of the loss function and the weight \( w_{i} \).
The loss function, then, needs to be differentiable, in which case the minimum can be reached via the backpropagation algorithm.
This algorithm employs the chain rule to calculate the gradient of the loss function with respect to the weights---a layer at a time, iterating backwards---and another algorithm, such as gradient descent, adjusts the weights by moving opposite to the gradient iteratively, such that the weight at iteration \( t + 1 \) is
\begin{equation}
    w_{i, t + 1} = w_{i, t} - \alpha \diffp{\mathcal{L}}{w_{i}},
\end{equation}
with \( \alpha \) the learning rate.
The learning rate is a hyperparameter, meaning it needs to be adjusted by means outside the neural network, and ensures that the steps taken do not overshoot the minimum; on the other hand, a too small learning rate will result in training taking longer than necessary, as more steps must be taken than needed.
The algorithm terminates at some point when the absolute value of the gradient is less than some preset tolerance.

Because neural networks these days tend to involve very large datasets, the application of gradient descent can be a slow process owing to the summing of the gradients of all training examples.
This burden can be reduced by applying stochastic gradient descent which calculates an approximation of the true gradient by sampling a subset of the training data at each iteration, done on batches of training samples, incidentally allowing application of parallelization techniques.

The problem of choosing the correct learning rate is solved by optimizers such as Adam~\cite{Kingma2014}, employing adaptive learning rates for each weight in the network and calculating the average of the first- and second moments of the gradient at each iteration.
These are then controlled using two hyperparameters as decay rates, \( \beta_{1} \) and \( \beta_{2} \), typically set at \( \beta_{1} = 0.9 \) and \( \beta_{2} = 0.99 \).

The backpropagation algorithm has been implemented in certain frameworks, most noteworthy TensorFlow and PyTorch, as differentiable programming by automatic differentiation, which allows the chain rule to be easily (and quickly!) employed.
This is either done by building a static graph of the network, compiled before running the program, and containing derivative information at all nodes\footnote{This is how TensorFlow 1.x works}, or operator overloading which defines derivatives of functions/tensors in the language itself\footnote{This is how PyTorch and TensorFlow 2.x work by default}.
This is the true enabling technology of the two frameworks, and some languages---such as Swift---even aim to include automatic differentiation as a first-class language feature giving users a type-safe way of running machine learning; it is an exciting time for ML.\@

While automatic differentiation enables the popular frameworks to run backpropagation efficiently, another key technology has helped neural networks (and ML in general) achieve a breakthrough in to 2010s: graphical processing units (GPUs).
These were first introduced to the mass market in the 1990s to accelerate and improve graphics performance of desktop computers, but their parallel nature has meant it is suited well for neural networks that require matrix multiplication, as we have seen.
GPUs are designed such that they are efficient at a small number of specific operations, in contrast to the more generalist nature of CPUs, and tend to have an enormous amount of computing cores relative to CPUs; a new Apple MacBook may have \num{8} CPU cores, whereas the latest desktop GPU from Nvidia sports \num{8704}.
Nvidia also provides an API for doing tensor calculations with its cores, CUDA, which is what PyTorch and TensorFlow interfaces with, such that the common data scientist can operate in familiar languages such as Python, instead of using C++ and CUDA, but it does mean that Nvidia, for the moment, has a near-monopoly on machine learning acceleration.

The toy linear regression example will converge to the best linear fit, but it is only useful for problems that are linearly separable; by its linear nature, it is only capable of linear transformations.
This rather limits the application of a perceptron, inasmuch as many real-world problems are non-linear with respect to the input variables, and this observation leads to the introduction of non-linear activation functions into the network.
Doing so actually allows a single layer neural network to approximate any continuous function, as shown in~\cite{Cybenko1989,Hornik1989}.
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.3\textwidth]{./images/machine_learning/approximation_function.png}
    \caption{Example of a wide neural network approximating the function \( f(x) = 0.2 + 0.4 x^{2} + 0.3 x \sin{\left( 15 x \right) + 0.05 \cos{\left( 50 x \right)}} \), taken from \url{http://neuralnetworksanddeeplearning.com/chap4.html}.}\label{fig:approximation_function}
\end{figure}
The results refer to \enquote{sigmoidal}, or \enquote{squashing}, functions, examples of which are the sigmoid and the rectifier shown in~\vref{fig:activation_functions}.
The result can be graphically understood by inspection of~\vref{fig:approximation_function}:
the composition of the activation functions by each neuron allows us to approximate the value of any continuous function, provided the network is wide enough; the result holds as well for multivariate functions.

Thus we end up with \enquote{multi-layer feedforward networks} which can approximate almost any function, which consist of a vector of inputs multiplied by a matrix (the weights connected to a neuron) with a bias added (all of which is an affine transformation) passed through a non-linear activation function.

The question then becomes, why are the networks of today \textit{deep} instead of \textit{wide}?
The answer is that universality has also been proved for deep networks~\cite{Lu2017}, and these types of network exhibit extremely useful properties that we can benefit from.

\section{Temporal convolutional neural networks}\label{sec:temporal_convolutional_neural_networks}

A problem arises, however, when we want to apply neural networks to images, something that is easily recognized as extremely useful (both for good and objectively bad purposes).
Say we flatten a \( 100 \times 100 \) image into a vector of length \num{10000}, and feed it to a multi-layer feedforward network; each neuron in the first layer will now have \num{10000} weights that need to be learned!
Additionally, the internal structure of the image is disregarded, in that there can be meaning to the height, width and channel axes.

This is remedied by introducing a linear transformation preserving the structure of the input: a convolution.
\enquote{Convolutional neural networks} (CNNs) create feature maps by employing discrete convolutions via kernels that slide over the input and aggregate information in the receptive field; see~\vref{fig:convolution}.
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\textwidth]{./images/machine_learning/convolution.png}
    \caption{An example of discrete convolutions. The dark blue \( 3 \times 3 \) kernel slides over the input, and sums the product of the weights and inputs forming a feature map (green matrix) in the process.
    Image from~\cite{Dumoulin2016}.}\label{fig:convolution}
\end{figure}
CNNs first came to prominence in the realm of image recognition primarily because they impose a sense of locality on the network which meshes well with images: there are probably local features that are important for the output, which could not be taken into account if the structure was lost by applying simple fully connected layers directly to the input; it is also wasteful, as the local information can probably be pooled and share weights.
Not incidentally, pooling layers are the second feature of CNNs that made them world famous.
These down-sample the output of a convolutional layer by (typically) taking the max value of some subset of values of the image\footnote{Pooling layers can also work by taking averages or some other metric.}, discarding information but serving to reduce parameter size.
% \begin{figure}
%      \centering
%      \begin{subfigure}[b]{0.4\textwidth}
%          \centering
%             \includegraphics[width=\textwidth]{./images/machine_learning/Conv_layer.png}
%             \caption{}\label{fig:conv_layer}
%      \end{subfigure}
%      \hfill
%      \begin{subfigure}[b]{0.4\textwidth}
%          \centering
%             \includegraphics[width=\textwidth]{./images/machine_learning/Max_pooling.png}
%             \caption{}\label{fig:max_pooling}
%      \end{subfigure}
%         \caption{First image from \url{https://en.wikipedia.org/wiki/File:Conv_layer.png}, second from \url{https://upload.wikimedia.org/wikipedia/commons/e/e9/Max_pooling.png}}\label{fig:conv_and_pool}
% \end{figure}

Standard CNNs have enjoyed enormous success over the past ten years, and many image applications were built using these.
Another type of networks, \enquote{recurrent neural networks} (RNNs), though, took the machine translation, speech recognition and time series prediction world with storm.
RNNs have storage, internal states that can be used for memory in series where connected events may happen at different intervals.
\todo[inline]{More about RNNs?}

Recently, however, it has been shown that a new type of CNN, \enquote{temporal convolutional networks} (TCNs)~\cite{Bai2018}, perform better in a wide variety of tasks once dominated by RNNs.
The million-dollar idea of TCNs are dilated convolutions, used to great effect in~\cite{Oord2016}.
Whereas discrete convolution is defined as
\begin{equation}
    F(s) = \left( \bm{x} * f \right)(s) = \sum_{i = 0}^{k - 1} f(i) \cdot \bm{x}_{s - i},
\end{equation}
where \( \bm{x} \in \mathbb{R}^{n} \) is an input sequence and \( f: \{ 0, \ldots, k - 1 \} \rightarrow \mathbb{R} \) a filter, the dilated convolution operator is defined as
\begin{equation}
    F(s) = \left( \bm{x} *_{d} f \right)(s) = \sum_{i = 0}^{k - 1} f(i) \cdot \bm{x}_{s - d \cdot i}.
\end{equation}
This, then, inserts steps of size \( d \) into the filter, and reduces to a regular convolution for \( d = 1 \).
The brilliance of dilated convolutions lies in the fact that it enables the receptive field---the locations in the input array higher level layers are connected to---to grow exponentially in size, while the parameters only grow linearly.
This is done by increasing the stride, \( d \), in subsequent layers, as shown in~\vref{fig:strides}: in (a) \( F_{1} \) is produced from \( F_{0} \), and each element has a receptive field of \( 3 \times 3 \).
In (b) \( F_{2} \) is produced from \( F_{1} \), and each element now has a receptive field of \( 7 \times 7 \).
Lastly, in (c), \( F_{3} \) is produced from \( F_{2} \), and each element has a receptive field of \( 15 \times 15 \).
However, \( F_{1} \) through \( F_{3} \) have the same number of parameters.
Thus when we use a larger dilation, an output at the top level can represent a wider range of inputs~\cite{Bai2018}.
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\textwidth]{./images/machine_learning/strides.png}
    \caption{(a): \( F_{1} \) is produced from \( F_{0} \), and each element has a receptive field of \( 3 \times 3 \).
    (b): \( F_{2} \) is produced from \( F_{1} \), and each element has a receptive field of \( 7 \times 7 \).
    (c): \( F_{3} \) is produced from \( F_{2} \), and each element has a receptive field of \( 15 \times 15 \).
    Image from~\cite{Yu2015}.}\label{fig:strides}
\end{figure}

TCNs are introduced as a new tool for sequence modelling tasks, and are built on Fully Convolutional Networks~\cite{Long2015}, distinguished by having all hidden layers be the same size of the input, employing zero padding of kernel size - 1 to ensure the correct output shape.
TCNs go a step further by only convolving with inputs from time \( t \) and earlier to ensure a sense of causality in the network; there is no leakage from the future into the past.
It is simple to tweak the convolutions, such that this causality is violated, and the network may use inputs from all time steps.
To aid deep network stability, residual connections~\cite{He2016} have been used.
These make the output dependent on the addition of the convolution to the identity, i.e.
\begin{equation}
    o = \sigma{\left[ \bm{x} + \mathcal{F}(\bm{x}) \right]},
\end{equation}
with \( \sigma \) the activation function, \( \bm{x} \) some input and \( \mathcal{F} \) the network transformation; now, the network learns a modification to the identity mapping.
It should be noted that the residual connection for a TCN is not directly added to the output, as this would have shape implications.
Instead, a \( 1 \times 1 \) convolution is applied, to ensure the correct size of the residual.
The TCN residual block consists of two dilated convolutions, a weight normalization~\cite{Salimans2016}---which parameterizes the weights in the network to facilitate faster learning---layer (not used in this work; batch normalization, explained shortly, was used instead) followed by a ReLU (Rectified Linear Unit) layer and a dropout layer.
The architecture can be seen in~\vref{fig:tcn}.
\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\textwidth]{./images/machine_learning/tcn.png}
    \caption{(a): a demonstration of dilated convolutions with dilation factor \( d \in \{ 1, 2, 4\} \) and filter size 3.
    The output is easily able to cover a large receptive field.
    (b): The TCN residual block, with two dilated convolutions each followed by normalization and activation.
    (c): The residual connection from input to output.
    Image from~\cite{Bai2018}.}\label{fig:tcn}
\end{figure}
Batch normalization~\cite{Ioffe2015} is applied in order to reduce dependence on the careful selection of correct learning rate, because in a deep network the input to any layer is dependent on previous layers, and so changes to any parameter balloons through the network.
Thus some sense of normalization that preserves the important structure but suppresses the scale is beneficial.

Batch normalization relies on the gradient of batches, briefly touched on earlier in the discussion on stochastic gradient descent.
One calculates the approximate gradient,
\begin{equation}
    \frac{1}{m} \diffp*{\mathcal{L}(\bm{x}_{i}, \Theta)}{\Theta},
\end{equation}
where \( m \) is the size of the batch, \( \bm{x}_{i} \) is the \( i \)'th training sample and \( \Theta \) the network parameters that will minimize the loss function \( \mathcal{L} \).
To normalize the output of a layer we calculate the mean and variance of the values of the mini-batch and normalize to those.
Furthermore, the values are shifted and scaled by the learnable parameters \( \gamma \) and \( \beta \).

The values are passed through a ReLU, a Rectified Linear Unit, shown in~\vref{fig:activation_functions}.
\todo[inline]{Remember to explain dropout}
In this thesis a TCN is employed, and a regular 1D convolutional network as well.
In a 1D CNN the kernel slides in only one direction, but is otherwise similar to image convolution networks.

\section{Loss functions}\label{sec:loss_functions}

The choice of loss function influences the performance of the network.
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\textwidth]{./images/machine_learning/loss.png}
    \caption{A complicated neural net loss landscape.
    Image from~\cite{Li2018}.}\label{fig:loss_landscape}
\end{figure}
Of course, the function needs to be differentiable, or backpropagation will fail.
\todo[inline]{Loss functions... what clever thing to say?}

\section{Pre-processing}\label{sec:data_transformation}

Pre-processing is ever important in the data science space.
Often datasets will contain missing, garbled or even complete nonsense values, and these must be pruned.
This step can be said for, the IceCube problem, to be handled by the processing levels described in~\vref{sec:post_processing} which cleans noise and events that do not look like data, but the pre-processing work does not stop there in neural network settings.
Because backpropagation relies on gradients, it is necessary these do not explode, or else training can proceed no further.
To manage the scale of the inputs, then, the trick employed by many is to simple re-scale the distributions.
This can of course be done by removing the mean and scale according to the variance, such that
\begin{equation}
    \text{transformed}(x_{i}) = \frac{x_{i} - \mu}{\sigma},
\end{equation}
with \( \mu \) the mean, \( \sigma \) the standard deviation and \( x_{i} \) the input feature of the \( i \)'th sample.
However, this method is sensitive to outliers in the data, which will skew both the mean and the standard deviation, so instead robust scaling can be employed which uses the median and the interquartile range (IQR) instead.
The IQR is the difference between the third and first quartiles of a dataset, that is the middle number between the minimum and the median, and the middle value between the median and the maximum respectively.
The robust scaler is then defined as
\begin{equation}
    \text{transformed}(x_{i}) = \frac{x_{i} - \text{median}}{\text{IQR}},
\end{equation}
and is implemented as such in e.g. \verb|scikit-learn|.
If the dataset is skewed it may be beneficial to first log transform it, before applying scaling, but there are also other non-linear methods, such as a quantile transformation, which transforms the data into a new distribution (e.g. Gaussian or uniform), robust to outliers.
However, as this is a non-linear transformation, it distorts any linear relationships between variables.


\end{document}
