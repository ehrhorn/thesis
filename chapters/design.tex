%!TEX root = ../main.tex
\documentclass[../main.tex]{subfiles}

\begin{document}

\section{Defining the problem}\label{sec:defining_the_problems}

With the given dataset, it is possible to train a neural network to predict the following properties of an incoming neutrino: energy, interaction vertex, interaction time and direction.
As the oscillation studies---as described in~\vref{sec:neutrino_oscillations}---depend on the energy and the polar angle (as a proxy for the length traveled through the Earth), these two variables are the ones examined in this work.

The algorithm performance will be evaluated based on \( \log_{10}(E) \) and \( \theta \) resolution in 18 energy bins in the range \SIrange{1}{1000}{\giga\electronvolt} of simulated charged current muon neutrinos \enquote{detected} in DeepCore, 6 bins per order of magnitude.
To gauge the performance the algorithm will be tested against the current best DeepCore reconstruction algorithm, Retro Reco (\vref{sec:l6}).

In the following sections first the custom made tooling used to solve the problem is described, whereafter hyperparameter- and model experiments are performed.

The full DeepCore, charged current muon neutrino dataset (\vref{sec:distributions_and_selections}) where Retro Reco reconstruction is available contains \num{7280939} events in total, where \num{2050125} are blinded, set aside as a test set, leaving \num{5230814} for training.
Of these \num{1046163} are used for validation, which means \num{4184651} are trainable.
To save time, most experiments were run on a set containing around \num{400000} events, and all experiments employ early stopping.

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \input{./images/design/run_loss.pgf}
         \caption{}\label{fig:run_loss}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=0.85\textwidth]{./images/design/1200px-Overfitting.svg.png}
         \caption{}\label{fig:overfitting}
     \end{subfigure}
        \caption{(a) shows an actual example of the training and validation loss values during an experiment.
        The two diverge before the 10th epoch with the validation loss staying somewhat flat, and the training loss decreasing.
        This indicates that the network has begun overfitting to the training data, an example of which is shown by the green line in (b) (from~\cite{WikiOverfitting}).
        This will lead to a decreased inference capacity of the network because it is unable to generalize to unseen data.}\label{fig:loss_and_overfitting}
\end{figure}

Early stopping runs an inference pass on the validation step at the end of each epoch, and records the loss function value.
When the loss is seen to improve on the training set, but not the validation set, it is a sign of overfitting to the training data, because the network is no longer able to infer well on unseen data.

\section{Tooling}\label{sec:tooling}

Having defined the problem, solving it requires finding an appropriate architecture.
Before that is possible, one needs the pipeline up and running, because training cannot proceed without the data structures defined and built, and evaluation cannot proceed without calculating the metrics and comparing one run to another.

While CubeDB was built for the creation of datasets, CubeFlow (\url{https://gitlab.com/ehrhorn/cubeflow}) served as the code for training and metric calculation.
Early on in the process, it was discovered that IceCube's event viewer, Steamshovel, while extremely powerful was difficult (and in fact, impossible on the author's available machines) to compile and run, it was decided to build an event viewer by using modern frameworks which supported data in the SQLite format from CubeDB.\@
The software was named Powershovel, a play on IceCube's native Steamshovel event viewer\footnote{A steam shovel is a power shovel; steam shovels were replaced by diesel-powered shovels in the 1930s}, and the source code is found at \url{https://gitlab.com/ehrhorn/powershovel}.
It was soon realized that more visualization tasks were needed, in addition to event viewing, and Powershovel was thus amended to contain two more sections, Distributions and Runs.

Distributions shows dataset variable histograms, calculated when a new set is created.
This is important for visual inspection of the variable distributions, to ensure nothing went wrong in the creation, to test if training and validation samples are similar, and to confirm that distributions are not changed by the transformations described in~\vref{sec:data_transformation}; a screenshot of the distribution screen, showing the variable \verb|energy_log10|, can be seen in~\vref{fig:powershovel_1}.

\begin{figure}
    \centering
    \includegraphics[width=1.0\textwidth]{./images/design/powershovel_1.png}
    \caption{The true energy distribution of a DeepCore dataset shown in Powershovel.
    Note the ability to chose transformation (raw or scaled), table (features or truth) and variable.}\label{fig:powershovel_1}
\end{figure}

The histograms are calculated and saved in \verb|pickle| files during dataset creation such that the tool is actually responsive and useful, and can load a new dataset---and a new variable---in milliseconds.
Powershovel is made using Python, Streamlit and Plotly to ensure wide support, modern web technologies and ease of use.

\begin{figure}
    \centering
    \includegraphics[width=1.0\textwidth]{./images/design/powershovel_2.png}
    \caption{The Powershovel event viewer page.
    Each light pulse is indicated by the colored sphere, superimposed on the black DOMs.
    The size of the pulse is proportional to its charge, while the color is decided by the relative time offset of the pulse.
    The user can see relevant reconstructions, such as direction or vertex position, if any are available for the dataset, while the events are binned in energy.
    The user may also filter on reconstruction metrics; as shown in the screenshot, this view has been filtered on showing events where the absolute value of the zenith reconstruction error is \SIrange{18.97}{40.6}{\degree}.}\label{fig:powershovel_2}
\end{figure}

\Vref{fig:powershovel_2} shows the event viewer page where a user may inspect events from different SQLite datasets.
This uses a subset of events from the true dataset database, in order to load quickly, an issue because all events need to be loaded up front, such that the user may filter on e.g.\ true energy of the event.
The event view is static, with the time dimension instead indicated by color, but the 3D plot is interactive such that the user may pan and zoom to view the event from different angles.
This page was also adapted so that it is possible to upload I3 files for viewing; it was spurred on by the IceCube group at The Niels Bohr Institute having trouble with Steamshovel, and so Powershovel was amended and sent to them for evaluation.

\begin{figure}
    \centering
    \includegraphics[width=1.0\textwidth]{./images/design/powershovel_3.png}
    \caption{The Runs page showing two different runs, side by side.
    This shows the zenith resolution, as detailed in~\vref{sec:metrics}.
    Notice the \enquote{Rel.\ imp.} in the bottom of each figure, showing the relative change in performance between the two runs.}\label{fig:powershovel_3}
\end{figure}

The Runs page shows the result of a training run.
After training, CubeFlow runs reconstructions on a given dataset, saves them to a SQLite database, calculates errors and metrics (see~\vref{sec:metrics}), makes histograms, and stores them in \verb|pickle| files for fast retrieval.
The runs have a common naming convention, which includes certain key indicators of the algorithm (e.g.\ maximum event length, loss function, etc.) which is then used for filtering purposes such that the user can quickly see all algorithms using for example an MSE loss function.
Multiple filters can be applied simultaneously.

\begin{figure}
    \centering
    \includegraphics[width=1.0\textwidth]{./images/design/powershovel_4.png}
    \caption{The Runs screen, further down the page than shown in~\vref{fig:powershovel_3}.
    Two 2D histogram plots are shown, true zenith vs.\ the reconstructed zenith.
    The energy bin selection is shown in the bottom left corner, and below the 2D histograms 1D histograms showing the distribution of zenith predictions in the selected energy bin can be seen.
    Furthermore, to highlight the interactive nature of Plotly (the framework drawing all plots in Powershovel) extra info is visible the left 2D histogram, a consequence of the cursor hovering over it.
    This allows the user to see values for every point in the histogram.
    Zooming and panning is also possible.}\label{fig:powershovel_4}
\end{figure}

The page has a two-column layout, which accommodates side-by-side comparison of two runs.
This is crucial, as there is no single figure of merit\footnote{Resolution is used for evaluating different algorithms, but it is not a scalar. See~\vref{sec:metrics}} summarizing the performance in one number.
Most figures have a subfigure below it, showing the data basis for calculating that figure;
and because figures are binned in energy, a slider allows the user to choose what energy bin this subfigure should show data from.
This can be seen in~\vref{fig:powershovel_4}.

The creation of this tool and CubeDB represents a large part of the man-hours of this project, but the tool was used extensively during development of the machine learning algorithm, detailed in~\vref{sec:algorithm}.
Although not written by a computer science graduate, nor a professional programmer, the code is shared freely in the hope that any idea is taken up and implemented by IceCube.

\section{Metrics}\label{sec:metrics}

\begin{figure}
    \centering
    \input{./images/design/error_distribution.pgf}
    \caption{Error resolution in energy bins.
    Top row shows the error distribution in energy bins [1.0, 1.2) (left) and [1.7, 1.8) }\label{fig:error_resolution_1}
\end{figure}

To define the performance of an algorithm the resolution is chosen.
The resolution is here defined as the normalized interquartile range of the error distribution in a certain energy bin, i.e.
\begin{equation}
    w \approx \frac{\text{IQR}}{1.349},
\end{equation}
an example of which is shown in the top row of~\vref{fig:error_resolution_1}.
The normalization is chosen because for a normal distribution
\begin{equation}
    \text{IQR} \approx 1.349 \sigma,
\end{equation}
meaning that for a normal distribution \( w \) would be \( \sigma \).

The error distribution is different for different metrics, but represents how well the algorithm reconstructs a neutrino; it is zero for perfect reconstruction.

For error distributions that are bounded, the 68th percentile is used instead, also shown in~\vref{fig:error_resolution_1}.

To give some sort of error estimate on \( w \) bootstrapping is employed with the BCa method~\cite{Efron1987} to give a \SI{90}{\percent} confidence interval.
The IQR reported by energy bin and its confidence interval can be seen in e.g.~\vref{fig:powershovel_4}.

\section{Algorithm}\label{sec:algorithm}

The best algorithm is chosen by essentially running experiments of different neural network designs.
Early experiments used a simple 1D CNN, which slides a kernel across the data in one direction (hence the name), followed by fully connected layers.
Later experiments employed TCNs, as these were seen to much improve the performance with a similar amount of parameters and similar performance with much fewer parameters.

Because the inner workings of a neural network is somewhat opaque, it can be difficult to reason about the design with supreme confidence, and so for new tasks\footnote{Staples of machine learning, such as the MNIST task, have years of experimentation behind it; in that sense it is an old task, contrary to the one in this thesis} experimenting is the only way to reason about the most effective design.

\subsection{Hyperparameters}

There is also the case of hyperparameters.
Not only must one choose an appropriate loss function, general values such as the learning rate need also be tuned, and even the shape of the scheduler that controls the change of learning rate over time.
Furthermore problem specific parameters may arise, such as the maximum length of an event in the case of IceCube reconstruction, necessary because tensors going into the neural network must have the same shape, meaning zero-padding to some common value is required\footnote{Even the nature of the zero-padding must be decided; do you zero-pad the end of the arrays, or maybe both ends (with the detector data in the middle)?}.

The data itself also comes in different flavors, in a sense; one may choose the collection of pulses in an event where SRT cleaning (used in level 2 cleaning, but can be implemented at any stage; see~\vref{sec:l2}) has been applied, which removes light that is not causally connected to the event, and must thus be considered noise.

Experiments were run on smaller sets (around \num{200000} events) to facilitate quick turnaround under the assumption that adding more data will improve performance incrementally.
The choice of learning rate was done via a learning rate finder, or \enquote{the Learning Rate Range Test} (LRRT)~\cite{Smith2017}, an attempt at automating the process.

\begin{figure}
    \centering
    \includegraphics[width=1.0\textwidth]{./images/design/lr_finder.png}
    \caption{An example of an LRRT run.
    At first, the learning rate is too low, and the loss is plateauing.
    In the optimal range, the loss descends, until the loss becomes unstable and eventually explodes.
    Figure from~\cite{LearningRateScan}.}\label{fig:lrrt}
\end{figure}

LRRT runs a learning rate scan---between two values chosen manually by the user---over one epoch, increasing the learning rate between each mini-batch, the training loss is recorded at each step.
When the rate is too low, the training loss will plateau, then descent and finally---when the learning rate is too high---explode; the user should then choose the learning rate to be where the training loss is descendant, as that represents the most optimal range of learning rates.
An example is shown in~\vref{fig:lrrt}.

\subsection{Loss function}

\begin{figure}
    \centering
    \input{./images/design/zenith_performance_loss.pgf}
    \caption{The zenith resolution over \num{18} energy bins from \SIrange{1}{1000}{\giga\electronvolt}.
    The lines serve to guide the eye, as the plot becomes too busy as a scatter plot with more than two groups.
    Zenith performance on small DeepCore dataset with three different loss functions, MSE, MAE and logcosh.
    Logcosh consistently performs worse than MSE, while MSE and MAE are similar at energies above \SI{10}{\giga\electronvolt}.
    However, MSE seems to perform best at very low energy.}\label{fig:zenith_performance_loss}
\end{figure}

\begin{figure}
    \centering
    \input{./images/design/loss_functions.pgf}
    \caption{Loss functions MSE, MAE and logcosh.
    The outsize weight assignment of MSE is seen at values over \( x = 1 \), while the linear fashion and kink at \( x = \) is visible for MAE.\@
    Logcosh is smooth and does not assign outsize weight to outliers.}\label{fig:loss_functions}
\end{figure}

The choice of loss function has an impact on the performance of a model.
A handful were tested, with the usual suspects Mean Squared Error (MSE), Mean Absolute Error (MAE) and the logarithm of the hyperbolic cosine (\( \log[\cosh(x)] \), termed simply logcosh) performing the best, as shown in~\vref{fig:zenith_performance_loss} using the zenith regression task as an example.

The three loss functions---MSE, MAE and logcosh---are staples of machine learning, but perform their duties differently.
MSE,
\begin{equation}
    \text{MSE} = \frac{1}{n} \sum_{i = 1}^{n} {\left( y_{\text{true}} - y_{\text{reco}} \right)}^2,
\end{equation}
assigns loss quadratically, which means that outliers are assigned an outsize weight.
Here, \( n \) is the number of samples in a mini-batch, \( y_{\text{true}} \) the true value and \( y_{\text{reco}} \) the reconstructed value.
MAE,
\begin{equation}
    \text{MAE} = \frac{1}{n} \sum_{i = 1}^{n} \lvert y_{\text{true}} - y_{\text{reco}} \rvert,
\end{equation}
is an attempt to remedy this by assigning a linear weight.
It has a constant derivative almost everywhere, but is non-differentiable at \( y_{\text{true}} = y_{\text{reco}} \), so must be approximated by a smooth function at that point.

Logcosh loss,
\begin{equation}
    \text{logcosh} = \sum_{i = 1}^{n} \log[\cosh(y_{\text{true}} - y_{\text{reco}})],
\end{equation}
is differentiable everywhere, and not as sensitive to outliers, because \( \text{logcosh} \approx x^2 / 2 \) for small \( x \) and \( \text{logcosh} \approx \lvert x \rvert - \log(2) \) for large \( x \), avoiding the problems of MSE.\@

\subsection{Prediction type}

The prediction of angles\footnote{Or, rather, zenith, which is the quantity of importance for oscillation studies} can proceed in several ways.

Because the azimuthal angle wraps around at \( 2 \pi \), it introduces problems for the neural network because it cannot understand that e.g.\ \SI{6.27}{\radian} and \SI{0.01}{\radian} are points close to each other on the circle.
This may be remedied by predicting the \( x \), \( y \) and \( z \) components of the directional vector associated with an event, which has the benefit of not relying on circular statistics.
The loss function may even contain a penalty term ensuring that the network attempts to predict unit vectors.
This was implemented, but was not found to make a significant difference compared to no penalty.

\begin{figure}
    \centering
    \input{./images/design/zenith_performance_prediction_type.pgf}
    \caption{Zenith resolution performance for different prediction types.
    Only predicting the zenith coordinate of the neutrino is seen to perform best over the largest range of energies, while the prediction of unit vectors outperforms it at higher energies (\SI{100}{\giga\electronvolt} and above).}\label{fig:prediction_types}
\end{figure}

Another avenue is to only predict the zenith angle, which does not suffer from circular wraparound, as it is restricted to \( \left[ \SI{0}{\degree}, \SI{180}{\degree} \right] \).
This leads to another issue: is it possible to input the raw detector data---which contains Cartesian input coordinates for the DOM positions---and expect the neural network to be able to map it to a spherical coordinate system?

\begin{figure}
    \centering
    \input{./images/design/zenith_performance_input_type.pgf}
    \caption{The difference between inputting the original detector data, in Cartesian coordinates, to the network, or doing a spherical transform beforehand, when predicting the neutrino's zenith directional coordinate.
    The performance is similar.}\label{fig:input_types}
\end{figure}

This was tested, as the thesis was that converting the coordinates beforehand might lessen the burden on the neural network, and the result can be seen in~\vref{fig:input_types}.
The performance is similar, and goes to show the power of a universal approximator: the network will discover the correlations itself, even between coordinate transformations.

Lastly, it is possible to predict \textit{both} the azimuthal and polar angle in the same network; this was, however, seen to lead to a slight decrease in zenith resolution, as seen in~\vref{fig:prediction_types}.

\subsection{Cleaning}

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
            \includegraphics[width=\textwidth]{./images/design/uncleaned_event.png}
            \caption{}\label{fig:uncleaned_powershovel_event}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
            \includegraphics[width=\textwidth]{./images/design/cleaned_event.png}
            \caption{}\label{fig:cleaned_powershovel_event}
     \end{subfigure}
        \caption{Event ID 124492073, a \SI{35.8}{\giga\electronvolt} event shown in Powershovel with no SRT cleaning (\vref{fig:uncleaned_powershovel_event}) and with (\vref{fig:cleaned_powershovel_event}).
        This is a track-like muon neutrino event, the neutrino path indicated by the blue line.
        Each black dot represents a DOM, and the pulses are colored according to the \enquote{plasma} color map with yellower pulses being later in time than bluer pulses.
        The size of the pulse is proportional to its charge.
        As is evident, SRT cleaning removes a large amount of pulses in this example, and the cleaned event leaves a sense of direction in the pulse which can be inferred by a human eye, and surely is easier to infer for a neural network.
        Note that the color scale is different on the two plots; this is caused by a missing feature not yet implemented in Powershovel.}\label{fig:two_powershovel_events}
\end{figure}

\begin{figure}
    \centering
    \input{./images/design/zenith_performance_cleaning_type.pgf}
    \caption{Impact of cleaning on zenith performance.
    SplitInIcePulses indicates events with no extra pulses removed by SRT cleaning, whilst SRTInIcePulses is the opposite.
    There is a clear difference in performance, which should be expected as the SRT cleaning removes unphysical pulses.}\label{fig:cleaning_type}
\end{figure}

Every event includes extra SRT cleaning, which removes any pulses that are unphysical (see~\vref{sec:l2}).
SRT cleaning tends to remove a large amount of pulses---as is shown in~\vref{fig:two_powershovel_events}---which is only noise, and should thus tend to aid neural networks in training.
Networks were trained with/without SRT cleaning, and it was found to be indeed so; an example is shown in~\vref{fig:cleaning_type}.
As low energy events do not tend to leave long, finely articulated tracks in DeepCore, this cleaning improves the resolution as the network does not need to pay attention to noise.

\subsection{Maximum event length}

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \input{./images/design/event_length.pgf}
         \caption{}\label{fig:event_length}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \input{./images/design/zenith_performance_length.pgf}
         \caption{}\label{fig:performance_length}
     \end{subfigure}
        \caption{The distributions of event lengths with/without SRT cleaning (Raw/Cleaned) as KDEs are shown in~\vref{fig:event_length}.
        \Vref{fig:performance_length} shows the effect of different maximum event length on the performance of the network.
    For 200 and 400 the performance is similar, while somewhat decreased for 100.
    This may be caused by the limitation of a maximum length of 100 \enquote{throws away} too much event information.
    This is reinforced by the fact that at the highest energy bin the performance of 200 decreases, probably because higher energy events are typically longer, while the performances are very similar at low energies.}\label{fig:length}
\end{figure}

The tensors that are input into the neural network need to be of the same shape, and thus some padding is necessary.
All tensors are therefore padded up to some pre-set maximum length, and events that are longer than this limit are instead composed of a random subsample of the total event.

The median length of a raw event is 50 and 17 for SRT cleaned ones.
200 was chosen as the best performant maximum length, when training time, memory usage and resolution performance was taken into account; see~\vref{fig:length}.

\subsection{Array padding}

\begin{figure}
    \centering
    \input{./images/design/zenith_performance_padding.pgf}
    \caption{Network performance with different padding types.
    Because all input tensors are required to have the same shape, each event is zero-padded up to the maximum event length.
    \enquote{End} padding pads the end of the array, while the poorly named \enquote{middle} places the event in the middle, and pads both ends.
    The performance is similar for both.}\label{fig:padding}
\end{figure}

The style of zero-padding was also varied, although with no discernible effect as shown in~\vref{fig:padding}.
Either the event was padded with zeros after the last entry and up to the maximum length, or the event was put into the middle of the array, with both ends then padded.
The network, however, seems to understand that zeros indicate no pulse, as the performance is very similar between the two.

\subsection{Weighting}

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \input{./images/design/dev_genie_numu_cc_train_retro_005_weighted_distribution.pgf}
         \caption{}\label{fig:weighted_distribution}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \input{./images/design/zenith_performance_weight.pgf}
         \caption{}\label{fig:weighting_performance}
     \end{subfigure}
        \caption{\Vref{fig:weighted_distribution} shows the energy distribution of the largest DeepCore muon neutrino dataset.
        Superimposed is the weighted distribution, where each entry counts with the value of its weight instead of 1.
        The distribution has been weighted to a uniform distribution.
        \Vref{fig:weighting_performance} shows performance with/without re-weighting by energy bins.
    The tails naturally improve their performance, while the bulk performs worse.}\label{fig:weighting}
\end{figure}

It is common practice in both machine learning and high energy physics to re-weight a distribution.
In the case of machine learning, it is often done such that distribution imbalances are evened out,
and for IceCube most neutrinos are around the \SIrange{10}{100}{\giga\electronvolt} mark, as seen in~\vref{fig:weighted_distribution}.
Thus re-weighting to a uniform distribution is very useful for extremely low energy events, under \SI{10}{\giga\electronvolt}, although performance suffers most everywhere else; see~\vref{fig:weighting}.

\subsection{Model architecture}

\begin{figure}
    \centering
    \input{./images/design/zenith_performance_models.pgf}
    \caption{Performance of four different networks.
    The low-parameter TCN 1.0 performs promising vs. CNNs with orders of magnitude more parameters, but fails at higher energies; this is remedied by TCN 2.0, which trades low amount of parameters for performance.}\label{fig:models}
\end{figure}

The preceding discussion holds as well for zenith prediction as for energy prediction (bar prediction type and input type, which are not energy relevant).
The same goes for the design of the network; where performance increased on zenith prediction, performance increased on energy prediction.

Several designs have been tested out, both of the CNN and TCN variety.
The first working version, \enquote{CNN 1.0}, consists of five convolutional layers (kernel size 5), each doubling the number of filters starting with 32, using ReLUs, max pool and batch normalization in each block.
This is then fed through four fully connected layers until terminating in an output layer.

The network was found to much improve by removing most convolutional layers, with \enquote{CNN 2.0} only containing two of these, with two fully connected layers afterwards.
The difference in performance, seen in~\vref{fig:models}, is rather striking everywhere but the lowest energy bins.

CNN 1.0 is a large network, containing \num{5735009} trainable parameters, cut down to \num{1718241} in CNN 2.0.

\enquote{TCN 1.0} is a temporal convolutional network with one stack of 64 filters (kernel size 2), dilations (1, 2, 4, 8, 16, 32), ReLUs and causal padding.
It only has \num{92161} trainable parameters, yet performs as well as CNN 2.0 until around \SI{100}{\giga\electronvolt}.
Upping the filters to 256, the number of stacks to 2, and adding batch normalization results in \num{3038209} trainable parameters for \enquote{TCN 2.0}\footnote{More---many more!---network designs were trialed than two CNNs and two TCNs; for reasons of brevity, they are not discussed here.}, the winning network of the bunch.

These models are compared visually in~\vref{fig:models}.
With the best network---architecture and hyperparameters---in hand, the next task is to compare it to IceCube's current reconstruction algorithm, Retro Reco.

\end{document}
