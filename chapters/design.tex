%!TEX root = ../main.tex
\documentclass[../main.tex]{subfiles}

\begin{document}

\section{Defining the problem}\label{sec:defining_the_problems}

The problem to solve is that of neutrino reconstruction based on input IceCube detector data using neural networks.
This of course requires finding an appropriate architecture, but before that is possible, one needs the pipeline up and running, because training cannot proceed without the data structures defined and built, and evaluation cannot proceed without calculating the metrics and comparing one run to another.

While CubeDB was built for the creation of datasets, CubeFlow (\url{https://gitlab.com/ehrhorn/cubeflow}) served as the code for training and metric calculation.
Early on in the process, it was discovered that IceCube's event viewer, Steamshovel, while extremely powerful was difficult (and in fact, impossible on the author's available machines) to compile and run, it was decided to build an event viewer by using modern frameworks which supported data in the SQLite format from CubeDB.\@
The software was named Powershovel, a play on IceCube's native Steamshovel event viewer\footnote{A steam shovel is a power shovel; steam shovels were replaced by diesel-powered shovels in the 1930s}, and the source code is found at \url{https://gitlab.com/ehrhorn/powershovel}.
It was soon realized that more visualization tasks were needed, in addition to event viewing, and Powershovel was thus amended to contain two more sections, Distributions and Runs.

Distributions shows dataset variable histograms, calculated when a new set is created.
This is important for visual inspection of the variable distributions, to ensure nothing went wrong in the creation, to test if training and validation samples are similar, and to confirm that distributions are not changed by the transformations described in~\vref{sec:data_transformation}; a screenshot of the distribution screen, showing the variable \verb|energy_log10|, can be seen in~\vref{fig:powershovel_1}.
\begin{figure} [ht]
    \centering
    \includegraphics[width=1.0\textwidth]{./images/design/powershovel_1.png}
    \caption{The true energy distribution of a DeepCore dataset shown in Powershovel.
    Note the ability to chose transformation (raw or scaled), table (features or truth) and variable.}\label{fig:powershovel_1}
\end{figure}
The histograms are calculated and saved in \verb|pickle| files during dataset creation such that the tool is actually responsive and useful, and can load a new dataset---and a new variable---in milliseconds.
Powershovel is made using Python, Streamlit and Plotly to ensure wide support, modern web technologies and ease of use.
\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\textwidth]{./images/design/powershovel_2.png}
    \caption{The Powershovel event viewer page.
    Each light pulse is indicated by the colored sphere, superimposed on the black DOMs.
    The size of the pulse is proportional to its charge, while the color is decided by the relative time offset of the pulse.
    The user can see relevant reconstructions, such as direction or vertex position, if any are available for the dataset, while the events are binned in energy.
    The user may also filter on reconstruction metrics; as shown in the screenshot, this view has been filtered on showing events where the absolute value of the zenith reconstruction error is \SIrange{18.97}{40.6}{\degree}.}\label{fig:powershovel_2}
\end{figure}

\Vref{fig:powershovel_2} shows the event viewer page where a user may inspect events from different SQLite datasets.
This uses a subset of events from the true dataset database, in order to load quickly, an issue because all events need to be loaded up front, such that the user may filter on e.g.\ true energy of the event.
The event view is static, with the time dimension instead indicated by color, but the 3D plot is interactive such that the user may pan and zoom to view the event from different angles.
This page was also adapted so that it is possible to upload I3 files for viewing; it was spurred on by the IceCube group at The Niels Bohr Institute having trouble with Steamshovel, and so Powershovel was amended and sent to them for evaluation.

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{./images/design/powershovel_3.png}
    \caption{The Runs page showing two different runs, side by side.
    This shows the zenith resolution, as detailed in~\vref{sec:metrics}.
    Notice the \enquote{Rel.\ imp.} in the bottom of each figure, showing the relative change in performance between the two runs.}\label{fig:powershovel_3}
\end{figure}

The Runs page shows the result of a training run.
After training, CubeFlow runs reconstructions on a given dataset, saves them to a SQLite database, calculates errors and metrics (see~\vref{sec:metrics}), makes histograms, and stores them in \verb|pickle| files for fast retrieval.
The runs have a common naming convention, which includes certain key indicators of the algorithm (e.g.\ maximum event length, loss function, etc.) which is then used for filtering purposes such that the user can quickly see all algorithms using for example an MSE loss function.
Multiple filters can be applied simultaneously.

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{./images/design/powershovel_4.png}
    \caption{The Runs screen, further down the page than shown in~\vref{fig:powershovel_3}.
    Two 2D histogram plots are shown, true zenith vs.\ the reconstructed zenith.
    The energy bin selection is shown in the bottom left corner, and below the 2D histograms 1D histograms showing the distribution of zenith predictions in the selected energy bin can be seen.
    Furthermore, to highlight the interactive nature of Plotly (the framework drawing all plots in Powershovel) extra info is visible the left 2D histogram, a consequence of the cursor hovering over it.
    This allows the user to see values for every point in the histogram.
    Zooming and panning is also possible.}\label{fig:powershovel_4}
\end{figure}

The page has a two-column layout, which accommodates side-by-side comparison of two runs.
This is crucial, as there is no single figure of merit\footnote{Resolution is used for evaluating different algorithms, but it is not a scalar. See~\vref{sec:metrics}} summarizing the performance in one number.
Most figures have a subfigure below it, showing the data basis for calculating that figure;
and because figures are binned in energy, a slider allows the user to choose what energy bin this subfigure should show data from.
This can be seen in~\vref{fig:powershovel_4}.

The creation of this tool and CubeDB represents a large part of the man-hours of this project, but the tool was used extensively during development of the machine learning algorithm, detailed in~\vref{sec:algorithm}.
Although not written by a computer science graduate, nor a professional programmer, the code is shared freely in the hope that any idea is taken up and implemented by IceCube.

\section{Metrics}\label{sec:metrics}
\begin{figure}[h]
    \centering
    \input{./images/design/error_distribution.pgf}
    \caption{Error resolution in energy bins.
    Top row shows the error distribution in energy bins [1.0, 1.2) (left) and [1.7, 1.8) }\label{fig:error_resolution_1}
\end{figure}
To define the performance of an algorithm the resolution is chosen.
The resolution is here defined as the normalized interquartile range of the error distribution in a certain energy bin, i.e.
\begin{equation}
    w \approx \frac{\text{IQR}}{1.349},
\end{equation}
an example of which is shown in the top row of~\vref{fig:error_resolution_1}.
The normalization is chosen because for a normal distribution
\begin{equation}
    \text{IQR} \approx 1.349 \sigma,
\end{equation}
meaning that for a normal distribution \( w \) would be \( \sigma \).

The error distribution is different for different metrics, but represents how well the algorithm reconstructs a neutrino; it is zero for perfect reconstruction.

For error distributions that are bounded, the 68th percentile is used instead, also shown in~\vref{fig:error_resolution_1}.

To give some sort of error estimate on \( w \) bootstrapping is employed with the BCa method~\cite{Efron1987} to give a \SI{90}{\percent} confidence interval.
The IQR reported by energy bin and its confidence interval can be seen in e.g.~\vref{fig:powershovel_4}.

\section{Algorithm}\label{sec:algorithm}

The best algorithm is chosen by essentially running experiments of different neural network designs.
Early experiments used a simple 1D CNN, which slides a kernel across the data in one direction (hence the name), followed by fully connected layers.
Later experiments employed TCNs, as these were seen to much improve the performance even with fewer parameters.

Because the inner workings of a neural network is somewhat opaque, it can be difficult to reason about the design with supreme confidence, and so for new tasks\footnote{Staples of machine learning, such as the MNIST task, have years of experimentation behind it; in that sense it is an old task, contrary to the one in this thesis} experimenting is the only way to reason about the most effective design.

There is also the case of hyperparameters.
Not only must one choose an appropriate loss function, general values such as the learning rate need also be tuned, and even the shape of the scheduler that controls the change of learning rate over time.
Furthermore problem specific parameters may arise, such as the maximum length of an event in the case of IceCube reconstruction, necessary because tensors going into the neural network must have the same shape, meaning zero-padding to some common value is required\footnote{Even the nature of the zero-padding must be decided; do you zero-pad the end of the arrays, or maybe both ends (with the detector data in the middle)?}.

The data itself also comes in different flavors, in a sense; one may choose the collection of pulses in an event where SRT cleaning (used in level 2 cleaning, but can be implemented at any stage; see~\vref{sec:l2}) has been applied, which removes light that is not causally connected to the event, and must thus be considered noise.

Experiments were run on smaller sets (around \num{200000} events) to facilitate quick turnaround under the assumption that adding more data will improve performance linearly.
The choice of learning rate was done via a learning rate finder, or \enquote{the Learning Rate Range Test} (LRRT)~\cite{Smith2017}, an attempt at automating the process.
\begin{figure}
    \centering
    \includegraphics[width=1.0\textwidth]{./images/design/lr_finder.png}
    \caption{An example of an LRRT run.
    At first, the learning rate is too low, and the loss is plateauing.
    In the optimal range, the loss descends, until the loss becomes unstable and eventually explodes.
    Figure from \url{https://www.jeremyjordan.me/nn-learning-rate/}.}\label{fig:lrrt}
\end{figure}
LRRT runs a learning rate scan---between two values chosen manually by the user---over one epoch, increasing the learning rate between each mini-batch, the training loss is recorded at each step.
When the rate is too low, the training loss will plateau, then descent and finally---when the learning rate is too high---explode; the user should then choose the learning rate to be where the training loss is descendant, as that represents the most optimal range of learning rates.
An example is shown in~\vref{fig:lrrt}.

\section{Loss function}

\begin{figure}
    \centering
    \input{./images/design/zenith_performance_loss.pgf}
    \caption{The zenith resolution over \num{18} energy bins from \SIrange{1}{1000}{\giga\electronvolt}.
    The lines serve to guide the eye, as the plot becomes too busy as a scatter plot with more than two groups.
    Zenith performance on small DeepCore dataset with three different loss functions, MSE, MAE and logcosh.
    Logcosh consistently performs worse than MSE, while MSE and MAE are similar at energies above \SI{10}{\giga\electronvolt}.
    However, MSE seems to perform best at very low energy.}\label{fig:zenith_performance_loss}
\end{figure}

\begin{figure}
    \centering
    \input{./images/design/loss_functions.pgf}
    \caption{Loss functions MSE, MAE and logcosh.
    The outsize weight assignment of MSE is seen at values over \( x = 1 \), while the linear fashion and kink at \( x = \) is visible for MAE.
    Logcosh is smooth and does not assign outsize weight to outliers.}\label{fig:loss_functions}
\end{figure}

The choice of loss function has an impact on the performance of a model.
A handful were tested, with the usual suspects Mean Squared Error (MSE), Mean Absolute Error (MAE) and the logarithm of the hyperbolic cosine (\( \log[\cosh(x)] \), termed simply logcosh) performing the best, as shown in~\vref{fig:zenith_performance_loss} using the zenith regression task as an example.

The three loss functions---MSE, MAE and logcosh---are staples of machine learning, but perform their duties differently.
MSE,
\begin{equation}
    \text{MSE} = \frac{1}{n} \sum_{i = 1}^{n} \left( y_{\text{true}} - y_{\text{reco}} \right)^2,
\end{equation}
assigns loss quadratically, which means that outliers are assigned an outsize weight.
Here, \( n \) is the number of samples in a mini-batch, \( y_{\text{true}} \) the true value and \( y_{\text{reco}} \) the reconstructed value.
MAE,
\begin{equation}
    \text{MAE} = \frac{1}{n} \sum_{i = 1}^{n} \lvert y_{\text{true}} - y_{\text{reco}} \rvert,
\end{equation}
is an attempt to remedy this by assigning a linear weight.
It has a constant derivative almost everywhere, but is non-differentiable at \( y_{\text{true}} = y_{\text{reco}} \), so must be approximated by a smooth function at that point.

Logcosh loss,
\begin{equation}
    \text{logcosh} = \sum_{i = 1}^{n} \log[\cosh(y_{\text{true}} - y_{\text{reco}})],
\end{equation}
is differentiable everywhere, and not as sensitive to outliers, because \( \text{logcosh} \approx x^2 / 2 \) for small \( x \) and \( \text{logcosh} \approx \lvert x \rvert - \log(2) \) for large \( x \), avoiding the problems of MSE.\@

\section{Prediction type}

The prediction of angles\footnote{Or, rather, zenith, which is the quantity of importance for oscillation studies} can proceed in several ways.

Because the azimuthal angle wraps around at \( 2 \pi \), it introduces problems for the neural network because it cannot understand that e.g.\ \SI{6.27}{\radian} and \SI{0.01}{\radian} are points close to each other on the circle.
This may be remedied by predicting the \( x \), \( y \) and \( z \) components of the directional vector associated with an event, which has the benefit of not relying on circular statistics.
The loss function may even contain a penalty term ensuring that the network attempts to predict unit vectors.
This was implemented, but was not found to make a significant difference.
\begin{figure}
    \centering
    \input{./images/design/zenith_performance_prediction_type.pgf}
    \caption{Zenith resolution performance for different prediction types.
    Only predicting the zenith coordinate of the neutrino is seen to perform best over the largest range of energies, while the prediction of unit vectors outperforms it at higher energies (\SI{100}{\giga\electronvolt} and above).}\label{fig:prediction_types}
\end{figure}
Another avenue is to only predict the zenith angle, which does not suffer from circular wraparound, as it is restricted to \( \left[ \SI{0}{\degree}, \SI{180}{\degree} \right] \).
This leads to another issue: is it possible to input the raw detector data---which contains Cartesian input coordinates for the DOM positions---and expect the neural network to be able to map it to a spherical coordinate system?
\begin{figure}
    \centering
    \input{./images/design/zenith_performance_input_type.pgf}
    \caption{The difference between inputting the original detector data, in Cartesian coordinates, to the network, or doing a spherical transform beforehand, when predicting the neutrino's zenith directional coordinate.
    The performance is similar.}\label{fig:input_types}
\end{figure}
This was tested, as the thesis was that converting the coordinates beforehand might lessen the burden on the neural network, and the result can be seen in~\vref{fig:input_types}.
The performance is similar, and goes to show the power of a universal approximator: the network will discover the correlations itself, even between coordinate transformations.

Lastly, it is possible to predict \textit{both} the azimuthal and polar angle in the same network; this was, however, seen to lead to a slight decrease in zenith resolution, as seen in~\vref{fig:prediction_types}.

\section{Cleaning}

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
            \includegraphics[width=\textwidth]{./images/design/uncleaned_event.png}
            \caption{}\label{fig:uncleaned_powershovel_event}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
            \includegraphics[width=\textwidth]{./images/design/cleaned_event.png}
            \caption{}\label{fig:cleaned_powershovel_event}
     \end{subfigure}
        \caption{Event ID 124492073, a \SI{35.8}{\giga\electronvolt} event shown in Powershovel with no SRT cleaning (\vref{fig:uncleaned_powershovel_event}) and with (\vref{fig:cleaned_powershovel_event}).
        This is a track-like muon neutrino event, the neutrino path indicated by the blue line.
        Each black dot represents a DOM, and the pulses are colored according to the \enquote{plasma} color map with yellower pulses being later in time than bluer pulses.
        The size of the pulse is proportional to its charge.
        As is evident, SRT cleaning removes a large amount of pulses in this example, and the cleaned event leaves a sense of direction in the pulse which can be inferred by a human eye, and surely is easier to infer for a neural network.}\label{fig:two_powershovel_events}
\end{figure}

\begin{figure}
    \centering
    \input{./images/design/zenith_performance_cleaning_type.pgf}
    \caption{Impact of cleaning on zenith performance.
    SplitInIcePulses indicates events with no extra pulses removed by SRT cleaning, whilst SRTInIcePulses is the opposite.
    There is a clear difference in performance, which should be expected as the SRT cleaning removes unphysical pulses.}\label{fig:cleaning_type}
\end{figure}

Every event includes extra SRT cleaning, which removes any pulses that are unphysical (see~\vref{sec:l2}).
SRT cleaning tends to remove a large amount of pulses, as is shown in~\vref{fig:two_powershovel_events} which is only noise, and should thus tend to aid neural networks in training.
Networks were trained with/without SRT cleaning, and it was found to be indeed so; an example is shown in~\vref{fig:cleaning_type}.
As low energy events do not tend to leave long, finely articulated tracks in DeepCore, this cleaning improves the resolution as the network does not need to pay attention to noise.

\begin{figure}
    \centering
    \input{./images/design/zenith_performance_length.pgf}
    \caption{}\label{fig:length}
\end{figure}

\begin{figure}
    \centering
    \input{./images/design/zenith_performance_padding.pgf}
    \caption{}\label{fig:padding}
\end{figure}

\end{document}
