%!TEX root = ../main.tex
\documentclass[../main.tex]{subfiles}

\begin{document}

If the neural network approach only performed as well, or better than Retro Reco, it would be a contender to replace it.
This is firstly because the inference speed, at \num{15000} events per second, is vastly improved compared to the minutes it takes Retro Reco to reconstruct an event.
Secondly, a neural network has less trouble generalizing to new detector states: whereas Retro Reco relies on some hypothesis and accompanying tables, the neural network simply needs data.
Thus where the old-school algorithm is simply unable to work on Upgrade data without a re-write, CubeFlow would only need training on Upgrade MC data; a feat that can be accomplished in weeks if not days, if the code and hardware infrastructure is already in place.
Even though a bespoke, old-school physics-based algorithm might work better, a neural network ally might get the collaboration up and running quickly, while the algorithm is worked out.
As such, there is no reason \textit{not} to use neural networks in IceCube.

Although the network presented in this thesis is by no means perfect, it does show significant improvement at low energies, relevant for oscillation analysis.
The network itself is simple enough, so there are many avenues for improvement.
This work would be significantly helped by the right tooling, such as CubeDB and Powershovel, because groups working on different algorithms would be able to compare on even terms, and use the same data with common references (event IDs) resting assured that the same events are used for training and testing across the board.

The scalability of neural networks is evident, as the Upgrade data was added and used as late as November; a working algorithm, hampered by noise, was up and running in mere minutes after the dataset was created, because it essentially only represents new data and the addition of new features (PMT directional vectors).
Added hardware sensitivity coupled with enhanced reconstruction through ML will allow the Upgrade to achieve, and even succeed, in its goal of world leading measurements of \( \sin^{2}{\theta_{2 3}} \) and \( \Delta{m_{3 2}^{2}} \) through muon neutrino survival and muon to tau oscillations at the oscillation extremum around \SI{25}{\giga\electronvolt}, a range where CubeFlow performs well even in DeepCore.

Supernova alarms is another case where the fast reconstruction of an ML algorithm is useful.
The promise of multi-messenger astronomy is the many faceted observation of cosmic events, and a GPU on the South Pole may be able to point towards something interesting in the sky with milliseconds of increased neutrino activity, alerting electromagnetic telescopes to start turning to look.

Of course the methods lack any validation in real data, a fact that might be remedied in time by observing the moon's shadow; because the moon stop most muons one would expect a deficit in its general direction, and the resolution can quantify the performance of the network.

In any case, neural networks have arrived in every area of physics, and can no longer be overlooked.
Newer methods, such as generative adversarial networks, are beginning to be used at the LHC as a supplement to MC data generation because it can generate new data from random noise by running a min-max game with a discriminator network; this reduces the computational burden of MC generation quite significantly.
This might also be of interest to IceCube, and as soon as neural networks are a staple of the collaboration toolbox, such ideas have an easier time being accepted.

\end{document}
