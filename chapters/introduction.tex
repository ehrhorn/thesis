%!TEX root = ../main.tex
\documentclass[../main.tex]{subfiles}

\begin{document}

Deep beneath the Amundsen-Scott station on the geographic South Pole, lurks a telescope in the ice.
This observatory hunts for some of the most elusive particles in our universe, hoping they may answer foundational questions about nature, and glean insight into new physics that the Genevan collider---or its potential successors---may never be able to.

The IceCube Neutrino Observatory was built between 2005 and 2010, but its heritage stretches back several decades.
Spanning one cubic kilometer of Antarctic ice, the telescope consists of some 60 strings, hot water drilled into the ice, with each string containing a number of photomultiplier tubes, capable of detecting photons originating from processes involving neutrinos---the aforementioned elusive particles.

Measurements of certain neutrino properties may help us understand phenomena such as why the universe is matter-dominated~\autocite{Chatterjee2013} and dark matter~\autocite{Baur2019}.
IceCube is also capable of searching for point-like sources~\autocite{Aartsen2020}, and serves in the SuperNova Early Warning System~\autocite{Kopke2011}.

But one thing is the ability to observe derived photons in Antarctic ice; another is reconstruction of this light in order to infer properties of the originating particles.
To date, classical methods, such as PegLeg~\autocite{Leuermann2018} and Retro Reco\footnote{\url{https://github.com/IceCubeOpenSource/retro}} have mostly been used for this purpose, but the burgeoning proliferation of the use of machine learning (ML) for scientific discovery~\autocite{Raghu2020} inspires the development of new methods for this cause.

Whereas classical methods require a hypothesis and thus preset logic, ML \enquote{changes these [problems] from logic problems to statistical problems}\footnote{\url{https://www.ben-evans.com/benedictevans/2019/9/6/face-recognition}}.
That is to say, given a large enough---or rather, a representative enough---dataset, an ML algorithm may find statistical similarities in disparate neutrino events sharing similar underlying properties, giving the network an ability to predict these same properties for previously unseen data.
As a \enquote{machine learning model is almost like a pure function at inference time}\footnote{Brennan Saeta, Software Engineer - TensorFlow, \url{https://www.swiftbysundell.com/podcast/58/}}, owing to the universal approximation theorems, it should be possible to construct an ML reconstruction algorithm that is both faster, preciser, more extensible, captures a wider spectrum and is more future-proof than the current methods.

The intention of this thesis is to explore that proposition, specifically for up-going muon neutrinos in the sub-teraelectronvolt range.

This problem is selected as it is the focus of the oscillations group at IceCube, studying neutrino oscillations.
The current best reconstruction algorithm for this case, Retro Reco, can take minutes to reconstruct a neutrino, while an ML-based approach can be expected to reconstruct several thousand each second.
An additional focus will be the IceCube upgrade~\autocite{Ishihara2019}, which will install additional improved detectors, and for which there is no current functioning reconstruction algorithm.

\section{Flow}

To weave a common thread through this document, and ensure the reader is informed about the big picture throughout, the following section presents the flow of the thesis.

The story of an interaction in IceCube from physics to analysis proceeds at a very high level as follows:
The universe is created with mechanisms that only allow a certain chirality of neutrinos to interact with forces other than gravity (\vref{sec:the_weak_interaction}).
At some point an interaction will create such a particle (\vref{sec:charged_and_neutral_currents}), which oscillates between different flavors (\vref{sec:neutrino_oscillations}).
As chance allows some neutrinos to interact in or around the instrumented ice, new particles are created that in turn send out a specific form of light (\vref{sec:cherenkov_radiation}) which the photomultiplier tubes (\vref{sec:doms}) can detect.
Today each in-ice digital optical module contains one photomultiplier tube.
With the IceCube upgrade this changes (\vref{sec:icecube_upgrade}), and new optical modules will be fitted with additional photomultiplier tubes which should help with determining the direction of neutrinos.
The detector is set up with certain triggers in differing levels (\vref{sec:triggers}), to ensure that not just any light is recorded and saved to disk.
Even further processing is done, as the bandwidth from pole to America is quite low, and so care must be taken in the choice of what to actually send down the wires for analysis.
When the data arrives for analysis, different level of cleaning takes place to root out noise ( \vref{sec:post_processing} ).
This pipeline also attempts to classify event types, using decision trees, until finally algorithms---none of them machine learning based---reconstruct neutrino properties of interest such as energy and direction (\vref{sec:post_processing}), and the events at all levels of cleaning are saved in an IceCube proprietary format, called I3 (\vref{sec:i3_file_format}).

With this part done, we turn towards machine learning generalities.
First the basics of machine learning in general (\vref{sec:ml_basics}), then deep learning in particular (\vref{sec:deep_learning}).
Special attention is given to temporal convolutional neural networks (\vref{sec:temporal_convolutional_neural_networks}), as this is the architecture used.
Because loss functions are such an important part of any deep learning project, a section is devoted to this concept as well (\vref{sec:loss_functions}), and for the most part, before data can be meaningfully trained in a neural network, some form of data transformation is required (\vref{sec:data_transformation}).

The last part of the thesis concerns the work by the author.
The data format provided by IceCube is not machine learning friendly, so a new solution fixing those shortcomings was built (\vref{sec:sqlite}).
So that the reader is on the same page as the author, plots and metrics used for comparison are explained.
The opponent algorithm is detailed (\vref{sec:post_processing}), before the algorithm is laid out (\vref{sec:algorithm}).

\end{document}
